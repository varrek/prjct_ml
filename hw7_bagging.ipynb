{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMms33J4jeDZ"
   },
   "source": [
    "1. Use any binary classification dataset\n",
    "2. Define validation strategy and use it for all next steps without changes\n",
    "3. Train decision tree model and estimate performance on validation\n",
    "4. Train bagging model with decision tree as a base model and estimate performance on validation\n",
    "5. Write your own bagging implementation:\n",
    "  <br>5.1. Define init for our CustomBaggingClassifier\n",
    "  <br>5.2. Write fit as described in lecture: divide train data on n parts (`n_estimators` in CustomBaggingClassifier), train `base_estimator` on each part and save these models inside class\n",
    "  <br>5.3. For predictions we should use all saved models and combine their predictions (as voting)\n",
    "6. Compare performance of sklearn bagging model with your own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/titanic_train.csv')\n",
    "data.drop(columns=['Name', 'Fare', 'PassengerId', 'Cabin', 'Ticket', 'Embarked'], axis = 1, inplace=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['onehotencoder__Sex_female', 'onehotencoder__Sex_male',\n",
       "       'standardscaler__Age', 'standardscaler__SibSp',\n",
       "       'standardscaler__Parch', 'remainder__Pclass'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = ['Sex',]\n",
    "numerical_features = ['Age', 'SibSp', 'Parch']\n",
    "ct = make_column_transformer(\n",
    "        (OneHotEncoder(), categorical_features),\n",
    "        (StandardScaler(), numerical_features),\n",
    "        remainder='passthrough', verbose_feature_names_out=True)\n",
    "data_transformed = ct.fit_transform(data.iloc[:, 1:], y=data.Survived)\n",
    "ct.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onehotencoder__Sex_female</th>\n",
       "      <th>onehotencoder__Sex_male</th>\n",
       "      <th>standardscaler__Age</th>\n",
       "      <th>standardscaler__SibSp</th>\n",
       "      <th>standardscaler__Parch</th>\n",
       "      <th>remainder__Pclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.530377</td>\n",
       "      <td>0.524570</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571831</td>\n",
       "      <td>0.524570</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.254825</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.365167</td>\n",
       "      <td>0.524570</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.365167</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.640719</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>5.357890</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.185937</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.737041</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.254825</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.158503</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     onehotencoder__Sex_female  onehotencoder__Sex_male  standardscaler__Age  \\\n",
       "0                          0.0                      1.0            -0.530377   \n",
       "1                          1.0                      0.0             0.571831   \n",
       "2                          1.0                      0.0            -0.254825   \n",
       "3                          1.0                      0.0             0.365167   \n",
       "4                          0.0                      1.0             0.365167   \n",
       "..                         ...                      ...                  ...   \n",
       "709                        1.0                      0.0             0.640719   \n",
       "710                        0.0                      1.0            -0.185937   \n",
       "711                        1.0                      0.0            -0.737041   \n",
       "712                        0.0                      1.0            -0.254825   \n",
       "713                        0.0                      1.0             0.158503   \n",
       "\n",
       "     standardscaler__SibSp  standardscaler__Parch  remainder__Pclass  \n",
       "0                 0.524570              -0.505895                3.0  \n",
       "1                 0.524570              -0.505895                1.0  \n",
       "2                -0.551703              -0.505895                3.0  \n",
       "3                 0.524570              -0.505895                1.0  \n",
       "4                -0.551703              -0.505895                3.0  \n",
       "..                     ...                    ...                ...  \n",
       "709              -0.551703               5.357890                3.0  \n",
       "710              -0.551703              -0.505895                2.0  \n",
       "711              -0.551703              -0.505895                1.0  \n",
       "712              -0.551703              -0.505895                1.0  \n",
       "713              -0.551703              -0.505895                3.0  \n",
       "\n",
       "[714 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data_transformed, columns=ct.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8665278180237612\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"max_features\": [0.7, 0.8, 0.9], \n",
    "    \"max_samples\": [0.7, 0.8, 0.9], \n",
    "    \"base_estimator__max_depth\": range(1, 5, 1),\n",
    "    \"base_estimator__min_samples_leaf\": range(2, 10, 1),\n",
    "    \"base_estimator__min_samples_split\":  range(2, 10, 1),\n",
    "}\n",
    "skf = StratifiedKFold(shuffle=True, random_state=42)\n",
    "bg = BaggingClassifier(DecisionTreeClassifier(), random_state=42, n_estimators=25)\n",
    "r_grid_search = RandomizedSearchCV(bg, param_grid, scoring ='roc_auc', n_iter=20, cv=skf, random_state=42, n_jobs=10)\n",
    "r_grid_search = r_grid_search.fit(df, data.Survived)\n",
    "print(r_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3,\n",
       "                                                        min_samples_leaf=3,\n",
       "                                                        min_samples_split=3),\n",
       "                  max_features=0.8, max_samples=0.9, n_estimators=25,\n",
       "                  random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['onehotencoder__Sex_female', 'onehotencoder__Sex_male',\n",
       "       'standardscaler__Age', 'standardscaler__SibSp', 'standardscaler__Parch',\n",
       "       'remainder__Pclass'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('onehotencoder',\n",
       "                                                  OneHotEncoder(), ['Sex']),\n",
       "                                                 ('standardscaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['Age', 'SibSp', 'Parch'])])),\n",
       "                ('baggingclassifier',\n",
       "                 BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3,\n",
       "                                                                         min_samples_leaf=3,\n",
       "                                                                         min_samples_split=3),\n",
       "                                   max_features=0.8, max_samples=0.9,\n",
       "                                   n_estimators=25, random_state=42))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3,\n",
    "                                                        min_samples_leaf=3,\n",
    "                                                        min_samples_split=3),\n",
    "                  max_features=0.8, max_samples=0.9, n_estimators=25,\n",
    "                  random_state=42)\n",
    "model = make_pipeline(ct, bagging)\n",
    "model.fit(data.iloc[:, 1:], data.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/titanic_test.csv')\n",
    "age_avg = test['Age'].mean()\n",
    "test[['Age']] = test[['Age']].fillna(age_avg)\n",
    "PassengerId = test['PassengerId']\n",
    "y_pred = model.predict(test[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']])\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": PassengerId,\n",
    "        \"Survived\": y_pred\n",
    "    })\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/titanic_train.csv')\n",
    "data.drop(columns=['Name', 'Fare', 'PassengerId', 'Cabin', 'Ticket', 'Embarked'], axis = 1, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "categorical_features = ['Sex',]\n",
    "numerical_features = ['Age', 'SibSp', 'Parch']\n",
    "ct = make_column_transformer(\n",
    "        (OneHotEncoder(), categorical_features),\n",
    "        (StandardScaler(), numerical_features),\n",
    "        remainder='drop', verbose_feature_names_out=True)\n",
    "data_transformed = ct.fit_transform(data.iloc[:, 1:], y=data.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data.Survived.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "709    0\n",
       "710    0\n",
       "711    1\n",
       "712    1\n",
       "713    0\n",
       "Name: Survived, Length: 714, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "ZSixk8wXjZJZ"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.base import clone, BaseEstimator\n",
    "class CustomBaggingClassifier(BaseEstimator):\n",
    "    def __init__(self, base_estimator=DecisionTreeClassifier(), n_estimators=100, max_samples=15, max_features=20, max_depth=3):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "  \n",
    "    def fit(self, X, y):\n",
    "        for estimator in range(self.n_estimators):\n",
    "            \n",
    "            sample = np.random.choice(np.arange(len(X)),len(X))\n",
    "            X_train_b = X\n",
    "            y_train_b = y\n",
    "            \n",
    "            model = clone(self.base_estimator)\n",
    "            model.fit(X_train_b, y_train_b)\n",
    "            self.models.append(model)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_test_hats = np.empty((len(self.models), len(X)))\n",
    "        for i, model in enumerate(self.models):\n",
    "            y_test_hats[i] = model.predict(X)\n",
    "        \n",
    "        return stats.mode(y_test_hats)[0]\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        y_test_hats = np.empty((len(self.models), len(X)))\n",
    "        for i, model in enumerate(self.models):\n",
    "            y_test_hats[i] = model.predict(X)\n",
    "        \n",
    "        return stats.mode(y_test_hats)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagger = CustomBaggingClassifier(DecisionTreeClassifier(), 100, 15, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagger.fit(data_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "bagger = CustomBaggingClassifier(DecisionTreeClassifier(), 100, 15, 10, 3)\n",
    "X = data_transformed\n",
    "y = y_train\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "scores = []\n",
    "for train_ix, test_ix in kfold.split(X):\n",
    "    train_X, test_X = X[train_ix], X[test_ix]\n",
    "    train_y, test_y = y[train_ix], y[test_ix]\n",
    "    bagger.fit(train_X, train_y)\n",
    "    yhat =  bagger.predict(test_X)[0]\n",
    "    acc = accuracy_score(test_y, yhat)\n",
    "    # store score\n",
    "    scores.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7272727272727273,\n",
       " 0.8391608391608392,\n",
       " 0.8461538461538461,\n",
       " 0.8951048951048951,\n",
       " 0.9084507042253521]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8432286023835319"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Sex',]\n",
    "numerical_features = ['Age', 'SibSp', 'Parch',]\n",
    "ct = make_column_transformer(\n",
    "        (OneHotEncoder(), categorical_features),\n",
    "        (StandardScaler(), numerical_features),\n",
    "        remainder='drop', verbose_feature_names_out=True)\n",
    "test_transformed = ct.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = bagger.predict(test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": PassengerId,\n",
    "        \"Survived\": y_test_hat[0].astype(int)\n",
    "    })\n",
    "submission.to_csv('submission_custom.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onehotencoder__Sex_female</th>\n",
       "      <th>onehotencoder__Sex_male</th>\n",
       "      <th>standardscaler__Age</th>\n",
       "      <th>standardscaler__SibSp</th>\n",
       "      <th>standardscaler__Parch</th>\n",
       "      <th>standardscaler__Pclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.530377</td>\n",
       "      <td>0.524570</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>0.911232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571831</td>\n",
       "      <td>0.524570</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>-1.476364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.254825</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>0.911232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.365167</td>\n",
       "      <td>0.524570</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>-1.476364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.365167</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>0.911232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.640719</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>5.357890</td>\n",
       "      <td>0.911232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.185937</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>-0.282566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.737041</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>-1.476364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.254825</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>-1.476364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.158503</td>\n",
       "      <td>-0.551703</td>\n",
       "      <td>-0.505895</td>\n",
       "      <td>0.911232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     onehotencoder__Sex_female  onehotencoder__Sex_male  standardscaler__Age  \\\n",
       "0                          0.0                      1.0            -0.530377   \n",
       "1                          1.0                      0.0             0.571831   \n",
       "2                          1.0                      0.0            -0.254825   \n",
       "3                          1.0                      0.0             0.365167   \n",
       "4                          0.0                      1.0             0.365167   \n",
       "..                         ...                      ...                  ...   \n",
       "709                        1.0                      0.0             0.640719   \n",
       "710                        0.0                      1.0            -0.185937   \n",
       "711                        1.0                      0.0            -0.737041   \n",
       "712                        0.0                      1.0            -0.254825   \n",
       "713                        0.0                      1.0             0.158503   \n",
       "\n",
       "     standardscaler__SibSp  standardscaler__Parch  standardscaler__Pclass  \n",
       "0                 0.524570              -0.505895                0.911232  \n",
       "1                 0.524570              -0.505895               -1.476364  \n",
       "2                -0.551703              -0.505895                0.911232  \n",
       "3                 0.524570              -0.505895               -1.476364  \n",
       "4                -0.551703              -0.505895                0.911232  \n",
       "..                     ...                    ...                     ...  \n",
       "709              -0.551703               5.357890                0.911232  \n",
       "710              -0.551703              -0.505895               -0.282566  \n",
       "711              -0.551703              -0.505895               -1.476364  \n",
       "712              -0.551703              -0.505895               -1.476364  \n",
       "713              -0.551703              -0.505895                0.911232  \n",
       "\n",
       "[714 rows x 6 columns]"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/titanic_train.csv')\n",
    "data.drop(columns=['Name', 'Fare', 'PassengerId', 'Cabin', 'Ticket', 'Embarked'], axis = 1, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "categorical_features = ['Sex',]\n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Pclass']\n",
    "ct = make_column_transformer(\n",
    "        (OneHotEncoder(), categorical_features),\n",
    "        (StandardScaler(), numerical_features),\n",
    "        remainder='passthrough', verbose_feature_names_out=True)\n",
    "data_transformed = ct.fit_transform(data.iloc[:, 1:], y=data.Survived)\n",
    "df = pd.DataFrame(data_transformed, columns=ct.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(max_depth=5, max_features=2, min_samples_leaf=3,\n",
       "                        n_estimators=50, n_jobs=-1, random_state=42),\n",
       " 0.8249679897567221)"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "parameters = {'max_features': [2, 3, 4], 'min_samples_leaf': [1, 3, 5, 7], 'max_depth': [3, 4, 5,10,15]}\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "gcv = GridSearchCV(rfc, parameters, n_jobs=-1, cv=skf, verbose=1)\n",
    "gcv.fit(df, data.Survived)\n",
    "gcv.best_estimator_, gcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=5, max_features=2, min_samples_leaf=3,\n",
    "                        n_estimators=50, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, max_features=2, min_samples_leaf=3,\n",
       "                       n_estimators=50, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(df, data.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/titanic_test.csv')\n",
    "age_avg = test['Age'].mean()\n",
    "test[['Age']] = test[['Age']].fillna(age_avg)\n",
    "categorical_features = ['Sex',]\n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Pclass']\n",
    "PassengerId = test['PassengerId']\n",
    "ct = make_column_transformer(\n",
    "        (OneHotEncoder(), categorical_features),\n",
    "        (StandardScaler(), numerical_features),\n",
    "        remainder='drop', verbose_feature_names_out=True)\n",
    "test_transformed = ct.fit_transform(test)\n",
    "test_transformed =  pd.DataFrame(test_transformed, columns=ct.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = clf.predict(test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": PassengerId,\n",
    "        \"Survived\": y_test_hat.astype(int)\n",
    "    })\n",
    "submission.to_csv('submission_forest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bagging homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
